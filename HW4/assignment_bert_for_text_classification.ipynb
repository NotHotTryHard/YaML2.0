{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNtLJlW4v5VF"
   },
   "source": [
    "## Классификация текстов с использованием предобученных языковых моделей.\n",
    "\n",
    "В данном задании вам предстоит обратиться к задаче классификации текстов и решить ее с использованием предобученной модели BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "%matplotlib inline\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к набору данных SST-2. Holdout часть данных (которая понадобится вам для посылки) доступна по ссылке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-23 07:57:52--  https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51581 (50K) [text/plain]\n",
      "Saving to: ‘texts_holdout.json’\n",
      "\n",
      "texts_holdout.json  100%[===================>]  50.37K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-11-23 07:57:52 (851 KB/s) - ‘texts_holdout.json’ saved [51581/51581]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "texts_train = df[0].values[:5000]\n",
    "y_train = df[1].values[:5000]\n",
    "texts_test = df[0].values[5000:]\n",
    "y_test = df[1].values[5000:]\n",
    "with open(\"texts_holdout.json\") as iofile:\n",
    "    texts_holdout = json.load(iofile)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь остальной код предстоит написать вам.\n",
    "\n",
    "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device, loss_func=nn.CrossEntropyLoss()):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return preds.item()\n",
    "\n",
    "def predict_probas(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        #print(outputs)\n",
    "    return outputs[:, 1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "ds_train = TextClassificationDataset(texts_train, y_train, tokenizer, max_length)\n",
    "ds_val = TextClassificationDataset(texts_test, y_test, tokenizer, max_length)\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(ds_train) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Validation Accuracy: 0.8990\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89       917\n",
      "           1       0.89      0.92      0.90      1003\n",
      "\n",
      "    accuracy                           0.90      1920\n",
      "   macro avg       0.90      0.90      0.90      1920\n",
      "weighted avg       0.90      0.90      0.90      1920\n",
      "\n",
      "Epoch 2/2\n",
      "Validation Accuracy: 0.9089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90       917\n",
      "           1       0.91      0.92      0.91      1003\n",
      "\n",
      "    accuracy                           0.91      1920\n",
      "   macro avg       0.91      0.91      0.91      1920\n",
      "weighted avg       0.91      0.91      0.91      1920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, dl_train, optimizer, scheduler, device)\n",
    "    accuracy, report = evaluate(model, dl_val, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:58<00:00, 85.02it/s] \n",
      "100%|██████████| 1920/1920 [00:26<00:00, 71.60it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 90.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_res = []\n",
    "val_res = []\n",
    "test_res = []\n",
    "\n",
    "for text in tqdm(texts_train):\n",
    "    res = predict_probas(text, model, tokenizer, device, max_length)\n",
    "    train_res.append(res)\n",
    "\n",
    "for text in tqdm(texts_test):\n",
    "    res = predict_probas(text, model, tokenizer, device, max_length)\n",
    "    val_res.append(res)\n",
    "\n",
    "for text in tqdm(texts_holdout):\n",
    "    res = predict_probas(text, model, tokenizer, device, max_length)\n",
    "    test_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1920\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_res))\n",
    "print(len(val_res))\n",
    "print(len(test_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сдача взадания в контест\n",
    "Сохраните в словарь `out_dict` вероятности принадлежности к первому (положительному) классу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05855754017829895, 0.9679608345031738, 0.5833645462989807, 0.0058727082796394825, 0.012927518226206303, 0.24907049536705017, 0.955267608165741, 0.011078527197241783, 0.8589808940887451, 0.9913628101348877, 0.05769398808479309, 0.0042026531882584095, 0.8582097291946411, 0.9718685150146484, 0.01902637630701065, 0.9914076924324036, 0.554171621799469, 0.9869203567504883, 0.060488324612379074, 0.3637818694114685, 0.007221213076263666, 0.01263545174151659, 0.9869203567504883, 0.8103464841842651, 0.9946797490119934, 0.9687501192092896, 0.9424953460693359, 0.9835017323493958, 0.18136875331401825, 0.1997794210910797, 0.6646702885627747, 0.7050139307975769, 0.012012647464871407, 0.9750622510910034, 0.9927587509155273, 0.9914311766624451, 0.006818759720772505, 0.05381226912140846, 0.9462679624557495, 0.9934646487236023, 0.9732042551040649, 0.010739143937826157, 0.782086968421936, 0.12148739397525787, 0.02973640151321888, 0.07334636151790619, 0.8162181973457336, 0.5833645462989807, 0.989933431148529, 0.03774076700210571, 0.9865798354148865, 0.016545215621590614, 0.015686500817537308, 0.12602967023849487, 0.9918615818023682, 0.032451312988996506, 0.9585511684417725, 0.6721738576889038, 0.08568869531154633, 0.9914311766624451, 0.06978001445531845, 0.0326806865632534, 0.9865906238555908, 0.027921030297875404, 0.013839170336723328, 0.9886385798454285, 0.6133251786231995, 0.9659777879714966, 0.6818987727165222, 0.0746786892414093, 0.04546808823943138, 0.05034599453210831, 0.0042026531882584095, 0.006024250295013189, 0.9820661544799805, 0.9635505080223083, 0.9756805896759033, 0.928604781627655, 0.9905574321746826, 0.7943419218063354, 0.10673047602176666, 0.02130190096795559, 0.4577941596508026, 0.9770516753196716, 0.9868560433387756, 0.9850737452507019, 0.05382159352302551, 0.013035647571086884, 0.9888723492622375, 0.007693010848015547, 0.11035653948783875, 0.015011955052614212, 0.9821212291717529, 0.9831944108009338, 0.9086354374885559, 0.01918661594390869, 0.0063146245665848255, 0.02926303632557392, 0.9638699889183044, 0.13937462866306305, 0.9878078103065491, 0.9804062843322754, 0.9405889511108398, 0.045783042907714844, 0.037340495735406876, 0.15801945328712463, 0.040953584015369415, 0.9929821491241455, 0.5776801109313965, 0.9272897243499756, 0.9835177659988403, 0.01902637630701065, 0.8625690340995789, 0.593589186668396, 0.9168993830680847, 0.5565913319587708, 0.8975393176078796, 0.9849141240119934, 0.9836628437042236, 0.010727670043706894, 0.008102972991764545, 0.9415097236633301, 0.6233311295509338, 0.9820820689201355, 0.9905574321746826, 0.017440473660826683, 0.9764581918716431, 0.014292941428720951, 0.9790107011795044, 0.7799503803253174, 0.007202671375125647, 0.9914851784706116, 0.9923535585403442, 0.37620487809181213, 0.992512583732605, 0.9626540541648865, 0.9929821491241455, 0.032535064965486526, 0.9923319816589355, 0.9469802975654602, 0.09367120265960693, 0.9946537017822266, 0.18428142368793488, 0.8208566904067993, 0.9869720339775085, 0.9057648777961731, 0.013839170336723328, 0.9840277433395386, 0.975454568862915, 0.9855905771255493, 0.9888833165168762, 0.8223539590835571, 0.0868944451212883, 0.026888038963079453, 0.019130555912852287, 0.5329696536064148, 0.9824506640434265, 0.7943419218063354, 0.01222917903214693, 0.9686003923416138, 0.03774076700210571, 0.8710092902183533, 0.010595723986625671, 0.016483629122376442, 0.9951861500740051, 0.13287343084812164, 0.02115354873239994, 0.0068427277728915215, 0.9656528830528259, 0.9827877283096313, 0.0059058484621346, 0.00719970278441906, 0.047148317098617554, 0.05273809656500816, 0.05769398808479309, 0.9189555048942566, 0.6999474167823792, 0.976311981678009, 0.08323445171117783, 0.9572077393531799, 0.9488950371742249, 0.039374932646751404, 0.9810813069343567, 0.9764581918716431, 0.02068631909787655, 0.007392148952931166, 0.013761322014033794, 0.9470769166946411, 0.064824178814888, 0.9873005747795105, 0.005730086471885443, 0.019426731392741203, 0.11325652152299881, 0.15707886219024658, 0.6264138221740723, 0.9804149866104126, 0.011451538652181625, 0.9920420050621033, 0.9915128350257874, 0.05570659041404724, 0.870026707649231, 0.016483629122376442, 0.987608015537262, 0.004651762079447508, 0.019660277292132378, 0.9761530160903931, 0.9834710955619812, 0.9923319816589355, 0.9770224094390869, 0.01968933828175068, 0.008180417120456696, 0.01582578755915165, 0.00575458537787199, 0.021444544196128845, 0.055874068289995193, 0.01113968063145876, 0.8790145516395569, 0.9811456799507141, 0.6799835562705994, 0.9086354374885559, 0.9866328239440918, 0.9820660948753357, 0.23325428366661072, 0.9912882447242737, 0.011154001578688622, 0.10673047602176666, 0.8465811014175415, 0.991613507270813, 0.8589808940887451, 0.9881843328475952, 0.37620487809181213, 0.004094695206731558, 0.09621740132570267, 0.9469802975654602, 0.9585916996002197, 0.9494392275810242, 0.02926303632557392, 0.003711048746481538, 0.9431273937225342, 0.9739267230033875, 0.9833565950393677, 0.7048540115356445, 0.9887096285820007, 0.25687992572784424, 0.04110807552933693, 0.9897770285606384, 0.019426731392741203, 0.0146331787109375, 0.029094286262989044, 0.489032506942749, 0.5174351930618286, 0.9833565950393677, 0.14108549058437347, 0.019379345700144768, 0.04353368282318115, 0.6406119465827942, 0.9691449999809265, 0.9680535197257996, 0.9851406812667847, 0.969290554523468, 0.007244633510708809, 0.009629913605749607, 0.9537143111228943, 0.9871293902397156, 0.919593870639801, 0.9811456799507141, 0.9805518388748169, 0.010595723986625671, 0.37095406651496887, 0.9406461715698242, 0.12028349190950394, 0.6578048467636108, 0.9922258853912354, 0.4264870285987854, 0.003595279762521386, 0.00477467430755496, 0.9912795424461365, 0.9308228492736816, 0.06978001445531845, 0.02449398674070835, 0.4264870285987854, 0.9569097757339478, 0.0048767756670713425, 0.9257526993751526, 0.9728814363479614, 0.9940202236175537, 0.012443399988114834, 0.9808175563812256, 0.02130190096795559, 0.6559807658195496, 0.9876458048820496, 0.9677894115447998, 0.1848595142364502, 0.13940435647964478, 0.2410517930984497, 0.3047388792037964, 0.00719970278441906, 0.9870778322219849, 0.9766972064971924, 0.9847553372383118, 0.028709571808576584, 0.9602713584899902, 0.928604781627655, 0.902696430683136, 0.0233463104814291, 0.9655936360359192, 0.21625418961048126, 0.0058917333371937275, 0.9742581248283386, 0.9111213684082031, 0.989247739315033, 0.9756636619567871, 0.0868944451212883, 0.13937462866306305, 0.9627570509910583, 0.9770355224609375, 0.005496891215443611, 0.02594490349292755, 0.011111586354672909, 0.504500150680542, 0.9642761945724487, 0.04613057151436806, 0.8790145516395569, 0.7050139307975769, 0.9318879842758179, 0.28700491786003113, 0.00575458537787199, 0.7799503803253174, 0.9909895062446594, 0.9876458048820496, 0.010392650030553341, 0.0752430334687233, 0.011974720284342766, 0.9861178994178772, 0.9909817576408386, 0.3562218248844147, 0.9626540541648865, 0.9865358471870422, 0.9184422492980957, 0.004128745757043362, 0.33607542514801025, 0.9731619358062744, 0.9950466156005859, 0.009238148108124733, 0.006736602168530226, 0.9698638916015625, 0.006408225279301405, 0.9810212850570679, 0.9743155241012573, 0.8253391981124878, 0.44702601432800293, 0.5349128842353821, 0.4264870285987854, 0.9678807854652405, 0.09621740132570267, 0.9690476059913635, 0.9097528457641602, 0.9732042551040649, 0.018482616171240807, 0.02057003602385521, 0.011154001578688622, 0.0926959216594696, 0.5833645462989807, 0.9914435744285583, 0.9830080270767212, 0.009014434181153774, 0.9739267230033875, 0.01896088197827339, 0.013827747665345669, 0.347257524728775, 0.07898073643445969, 0.9915993213653564, 0.9952452778816223, 0.7868887782096863, 0.6850429773330688, 0.09963663667440414, 0.008102972991764545, 0.03415551781654358, 0.007426203694194555, 0.16770686209201813, 0.0063146245665848255, 0.9897770285606384, 0.014120030216872692, 0.9773396849632263, 0.9931046366691589, 0.9363793730735779, 0.026818472892045975, 0.9263869524002075, 0.6818987727165222, 0.5193979144096375, 0.9950177073478699, 0.017033394426107407, 0.9902498126029968, 0.9602713584899902, 0.20469465851783752, 0.004481352400034666, 0.9950466156005859, 0.9856688976287842, 0.013086634688079357, 0.9946193695068359, 0.9508392214775085, 0.9843140840530396, 0.030326930806040764, 0.13846778869628906, 0.8790145516395569, 0.011012470349669456, 0.9848440289497375, 0.33607542514801025, 0.9884055852890015, 0.026888038963079453, 0.4916570782661438, 0.9849141240119934, 0.9820660948753357, 0.006082951556891203, 0.047148317098617554, 0.9303451776504517, 0.4657193720340729, 0.9127415418624878, 0.03674304485321045, 0.965315043926239, 0.8252917528152466, 0.9731282591819763, 0.9894249439239502, 0.9626932740211487, 0.9682230353355408, 0.5164380669593811, 0.013761322014033794, 0.20185936987400055, 0.004334447905421257, 0.030666006729006767, 0.1848595142364502, 0.951852560043335, 0.8589808940887451, 0.08858286589384079, 0.10384820401668549, 0.9865358471870422, 0.5003973841667175, 0.9822936654090881, 0.9929821491241455, 0.9678807854652405, 0.5046361088752747, 0.020524030551314354, 0.004128745757043362, 0.011557931080460548, 0.5122195482254028, 0.9907957911491394, 0.9604963064193726, 0.03594711422920227, 0.013479456305503845, 0.9762684106826782, 0.9700531959533691, 0.9667574763298035, 0.981097936630249, 0.9932422041893005, 0.03415551781654358, 0.8975393176078796, 0.9945377707481384, 0.20314867794513702, 0.00815843977034092, 0.9810344576835632, 0.048207372426986694, 0.006860979367047548, 0.03774076700210571, 0.9728814363479614, 0.9611177444458008, 0.2301170974969864, 0.9127675890922546, 0.9771869778633118, 0.504860520362854, 0.9625569581985474, 0.008978646248579025, 0.8582097291946411, 0.07535035163164139, 0.12148739397525787, 0.7079359292984009, 0.9732042551040649, 0.048207372426986694, 0.986427366733551, 0.9846349358558655, 0.07195408642292023, 0.9869203567504883, 0.834627091884613, 0.04050601273775101, 0.9822090268135071, 0.9946687817573547, 0.9761530160903931, 0.03674304485321045, 0.9602713584899902, 0.9902498126029968, 0.9875320792198181, 0.08816656470298767, 0.12328043580055237, 0.9820660948753357, 0.3004295229911804, 0.4964730739593506, 0.0146331787109375, 0.9868077635765076, 0.9868560433387756, 0.017353305593132973, 0.9882118105888367]\n"
     ]
    }
   ],
   "source": [
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    'train': train_res,  # list of length 5000 with probas\n",
    "    'test': val_res,  # list of length 1920 with probas\n",
    "    'holdout': test_res,  # list of length 500 with probas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько `assert`'ов для проверки вашей посылки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(out_dict[\"train\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"train\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "    len(out_dict[\"train\"]) == 5000\n",
    "), \"The predicted probas list length does not match the train set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"test\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"test\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "    len(out_dict[\"test\"]) == 1920\n",
    "), \"The predicted probas list length does not match the test set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"holdout\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"holdout\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "    len(out_dict[\"holdout\"]) == 500\n",
    "), \"The predicted probas list length does not match the holdout set size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите код ниже для генерации посылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `./submission_dict_hw_text_classification_with_bert.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "FILENAME = \"./submission_dict_hw_text_classification_with_bert.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as iofile:\n",
    "    json.dump(out_dict, iofile)\n",
    "print(f\"File saved to `{FILENAME}`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_hw01_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
